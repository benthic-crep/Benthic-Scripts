sm.test<-ddply(subset(survey_master,METHOD=="CORALBELT_METHOD_E_F"),.(REGION,ISLAND,SEC_NAME,OBS_YEAR,REEF_ZONE,DEPTH_BIN),summarize,n=length(SITE))
write.csv(data.test,"tmp_sitedataQC.csv")
write.csv(sm.test,"tmp_sitemasterQC.csv")
#Subset just Forereef Sites & just target taxa
site.data.gen2<-subset(site.data.gen2,REEF_ZONE=="Forereef")
site.data.gen2<-subset(site.data.gen2,GENUS_CODE %in% c("ACSP", "MOSP", "PAVS", "POCS","POSP","SSSS"))
rich.data<-subset(rich.data,REEF_ZONE=="Forereef")
#Make sure you everything but forereef are dropped
table(site.data.gen2$REEF_ZONE,site.data.gen2$GENUS_CODE)
table(rich.data$REEF_ZONE)
#Set ANALYSIS_SCHEMA to STRATA and DOMAIN_SCHEMA to whatever the highest level you want estimates for (e.g. sector, island, region)
site.data.gen2$ANALYSIS_SCHEMA<-site.data.gen2$STRATANAME
site.data.gen2$DOMAIN_SCHEMA<-site.data.gen2$ISLAND
rich.data$ANALYSIS_SCHEMA<-rich.data$STRATANAME
rich.data$DOMAIN_SCHEMA<-rich.data$ISLAND
#Calculate metrics at Strata-level-We need to work on combining metrics into 1 function
#Create a vector of columns to subset for strata estimates
c.keep<-c("REGION","ISLAND","ANALYSIS_YEAR","ANALYSIS_SCHEMA","GENUS_CODE",
"n_h","N_h","D._h","SE_D._h","avp","SEprop","Y._h","SE_Y._h","CV_Y._h")
c.keep2<-c("REGION","ISLAND","ANALYSIS_YEAR","ANALYSIS_SCHEMA","GENUS_CODE",
"n_h","N_h","D._h","SE_D._h")
acdG_st<-Calc_Strata(site.data.gen2,"GENUS_CODE","AdColDen","Adpres.abs");acdG_st=acdG_st[,c.keep]
colnames(acdG_st)<-c("REGION","ISLAND","ANALYSIS_YEAR","Stratum","GENUS_CODE","n","Ntot","AdColDen","SE_AdColDen","Adult_avp","Adult_seprop","Adult_Abun","Adult_SE_Abun","Adult_CV")
odG_st<-Calc_Strata(site.data.gen2,"GENUS_CODE","Ave.od");odG_st=odG_st[,c.keep2]
colnames(odG_st)<-c("REGION","ISLAND","ANALYSIS_YEAR","Stratum","GENUS_CODE","n","Ntot","Ave.od","SE_Ave.od")
c.keep<-c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA","n_h","N_h","D._h","SE_D._h")
rich_st<-Calc_Strata_Cover_Rich(rich.data,"Richness");rich_st=rich_st[,c.keep]
colnames(rich_st)<-c("REGION","ISLAND","ANALYSIS_YEAR","Sector","Stratum","n","Ntot","Richness","SE_Richess")
#Double Check that revised pooling is adding up NH (total sites) correctly
head(acdG_st,20)
subset(sectors,ISLAND=="Kingman") #Insert whichever island/strata you want to check.
#Add in CalcAnalayis_strata & island
#Calculate Regional Estimates
acdG_is<-Calc_Domain(site.data.gen2,"GENUS_CODE","AdColDen","Adpres.abs")
acdG_is<-acdG_is[,c("ANALYSIS_YEAR","DOMAIN_SCHEMA","GENUS_CODE","n","Ntot","Mean_AdColDen","SE_AdColDen")]
odG_is<-Calc_Domain(site.data.gen2,"GENUS_CODE","Ave.od");odG_is<-odG_is[,c("ANALYSIS_YEAR","DOMAIN_SCHEMA","GENUS_CODE","n","Ntot","Mean_Ave.od","SE_Ave.od")]
rich_is<-Calc_Domain_Cover_Rich(rich.data,"Richness");colnames(rich_is)[colnames(rich_is)=="DOMAIN_SCHEMA"]<-"Sector"
MyMerge <- function(x, y){
df <- merge(x, y, by= c("REGION","ISLAND","ANALYSIS_YEAR","Stratum","GENUS_CODE","n","Ntot"), all.x= TRUE, all.y= TRUE)
return(df)
}
st.data.gen<-Reduce(MyMerge, list(acdG_st,odG_st))
colnames(st.data.gen)[colnames(st.data.gen)=="ANALYSIS_SCHEMA"]<-"Stratum"
write.csv(st.data.gen,"Demog_surveymaster_strata.csv")
MyMerge <- function(x, y){
df <- merge(x, y, by= c("ANALYSIS_YEAR","DOMAIN_SCHEMA","GENUS_CODE","n","Ntot"), all.x= TRUE, all.y= TRUE)
return(df)
}
is.data.gen<-Reduce(MyMerge, list(acdG_is,odG_is))
colnames(is.data.gen)[colnames(is.data.gen)=="DOMAIN_SCHEMA"]<-"Island"
write.csv(is.data.gen,"Demog_surveymaster_island.csv")
t<-read.csv("smtest.csv")
dir()
t<-read.csv("smtest.csv")
head(t)
t$FLAG<-ifelse(t$SM_den!=t$SurveyM_den,1,0)
head(t)
t[t$FLAG=="1",]
t$FLAG2<-ifelse(t$Ntot!=t$Ntot.1,1,0)
t[t$FLAG2=="1",]
site.data.gen2[site.data.gen2$ISLAND=="Maui"]
site.data.gen2[site.data.gen2$ISLAND=="Maui",]
site.data.gen2[site.data.gen2$ISLAND=="Maui"&site.data.gen2=="SSSS"&site.data.gen2$OBS_YEAR=="2016",]
site.data.gen2[site.data.gen2$ISLAND=="Maui"&site.data.gen2$GENUS_CODE=="SSSS"&site.data.gen2$OBS_YEAR=="2016",]
site.data.gen2$NH
site_data=site.data.gen2
site_data$GROUP=site_data$GENUS_CODE
site_data$METRIC=site_data$AdColDen
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
head(Dom_NH)
Dom_NH[Dom_NH$=="Maui"]
Dom_NH[Dom_NH$ISLAND=="Maui",]
Strata_NH$Dom_N_h<-Dom_NH$Dom_N_h[match(Strata_NH$DOMAIN_SCHEMA,Dom_NH$DOMAIN_SCHEMA)]# add Dom_N_h to schema dataframe
Strata_NH$w_h<-Strata_NH$N_h/Strata_NH$Dom_N_h # add schema weighting factor to schema dataframe
Strata_NH[Strata_NH$ISLAND=="Maui",]
strat.temp[strat.temp$ISLAND=="Maui",]
#Now add back the Analysis_Schema Nh and wh to site_data
site_data$N_h.as<-Strata_NH$N_h[match(site_data$ANALYSIS_SCHEMA,Strata_NH$ANALYSIS_SCHEMA)]
site_data$w_h.as<-Strata_NH$w_h[match(site_data$ANALYSIS_SCHEMA,Strata_NH$ANALYSIS_SCHEMA)]
#Calculate summary metrics at the stratum level (rolled up from site level)
Strata_roll=ddply(site_data,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,GROUP),summarize,
n_h=length(SITE),# No. of Sites surveyed in a Strata
N_h=median(N_h.as,na.rm=T),# Strata Area (as N 50x50 grids) - median allows you to pick 1 value
w_h=median(w_h.as,na.rm=T),# weigting factor for a given stratum- median allows you to pick 1 value
D._h=mean(METRIC,na.rm=T), # Mean of Site-Level metric in a Stratum
S1_h=var(METRIC,na.rm=T), #sample variance in metric between sites
varD._h=(1-(n_h/N_h))*S1_h/n_h, #Strata level  variance of mean density
nmtot=(N_h*250), #total possible area
th=10, #minimum sampling unit
Y._h=D._h*nmtot*th,#total colony abundance in stratum **corrected using diones code
varY._h=((nmtot^2)*varD._h*(th^2)), #variance in total abundance- corrected using diones code
SE_D._h=sqrt(varD._h),
CV_D._h=(SE_D._h/D._h)*100,
SE_Y._h=sqrt(varY._h),
CV_Y._h=(SE_Y._h/Y._h)*100,
avp=sum(PRES.ABS)/n_h,
var_prop=(n_h/(n_h-1)*avp*(1-avp)),
SEprop=sqrt(var_prop))
Strata_roll$M_hi=250 #define total possible transects in a site
Strata_roll=Strata_roll[,c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA","GROUP",
"M_hi","n_h","N_h","w_h",
"D._h","S1_h","varD._h","SE_D._h","CV_D._h",
"Y._h","varY._h","SE_Y._h","CV_Y._h","avp","var_prop","SEprop")]
#remove strata that have only 1 site because you can't calculate variance
Strata_roll<-Strata_roll[Strata_roll$n_h>1,]
site_data$PRES.ABS=site_data$Adpres.abs
Strata_roll=ddply(site_data,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,GROUP),summarize,
n_h=length(SITE),# No. of Sites surveyed in a Strata
N_h=median(N_h.as,na.rm=T),# Strata Area (as N 50x50 grids) - median allows you to pick 1 value
w_h=median(w_h.as,na.rm=T),# weigting factor for a given stratum- median allows you to pick 1 value
D._h=mean(METRIC,na.rm=T), # Mean of Site-Level metric in a Stratum
S1_h=var(METRIC,na.rm=T), #sample variance in metric between sites
varD._h=(1-(n_h/N_h))*S1_h/n_h, #Strata level  variance of mean density
nmtot=(N_h*250), #total possible area
th=10, #minimum sampling unit
Y._h=D._h*nmtot*th,#total colony abundance in stratum **corrected using diones code
varY._h=((nmtot^2)*varD._h*(th^2)), #variance in total abundance- corrected using diones code
SE_D._h=sqrt(varD._h),
CV_D._h=(SE_D._h/D._h)*100,
SE_Y._h=sqrt(varY._h),
CV_Y._h=(SE_Y._h/Y._h)*100,
avp=sum(PRES.ABS)/n_h,
var_prop=(n_h/(n_h-1)*avp*(1-avp)),
SEprop=sqrt(var_prop))
Strata_roll$M_hi=250 #define total possible transects in a site
Strata_roll=Strata_roll[,c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA","GROUP",
"M_hi","n_h","N_h","w_h",
"D._h","S1_h","varD._h","SE_D._h","CV_D._h",
"Y._h","varY._h","SE_Y._h","CV_Y._h","avp","var_prop","SEprop")]
#remove strata that have only 1 site because you can't calculate variance
Strata_roll<-Strata_roll[Strata_roll$n_h>1,]
site_data[site_data$ISLAND=="Maui"&site_data$GENUS_CODE=="SSSS"&site_data$OBS_YEAR=="2016",]
Strata_NH[Strata_NH$ISLAND=="Maui",]
head(site_data)
site_data[site_data$ISLAND=="Maui"&site_data$GENUS_CODE=="SSSS"&site_data$OBS_YEAR=="2016",]
Strata_NH[Strata_NH$ISLAND=="Maui",]
strat.temp[strat.temp$ISLAND=="Maui",]
site_data[site_data$ISLAND=="Maui",]
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
Strata_NH$Dom_N_h<-Dom_NH$Dom_N_h[match(Strata_NH$DOMAIN_SCHEMA,Dom_NH$DOMAIN_SCHEMA)]# add Dom_N_h to schema dataframe
Strata_NH$w_h<-Strata_NH$N_h/Strata_NH$Dom_N_h # add schema weighting factor to schema dataframe
#Now add back the Analysis_Schema Nh and wh to site_data
site_data$N_h.as<-Strata_NH$N_h[match(site_data$ANALYSIS_SCHEMA,Strata_NH$ANALYSIS_SCHEMA)]
site_data$w_h.as<-Strata_NH$w_h[match(site_data$ANALYSIS_SCHEMA,Strata_NH$ANALYSIS_SCHEMA)]
site_data$Dom_N_h<-Strata_NH$Dom_N_h[match(site_data$ANALYSIS_SCHEMA,Strata_NH$ANALYSIS_SCHEMA)]
site_data[site_data$ISLAND=="Maui",]
site_data[site_data$ANALYSIS_SCHEMA=="MAI_SI_A",]
?match
site_data$N_h.as<-Strata_NH$N_h[match(site_data$ANALYSIS_SCHEMA,Strata_NH$ANALYSIS_SCHEMA,site_data$ANALYSIS_YEAR,Strata_NH$ANALYSIS_YEAR)]
site_data[site_data$ANALYSIS_SCHEMA=="MAI_SI_A",]
Strata_NH[Strata_NH$ANALYSIS_SCHEMA=="MAI_SI_A",]
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h.as=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h.as,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
Strata_NH$Dom_N_h<-Dom_NH$Dom_N_h[match(Strata_NH$DOMAIN_SCHEMA,Dom_NH$DOMAIN_SCHEMA)]# add Dom_N_h to schema dataframe
Strata_NH$w_h<-Strata_NH$N_h/Strata_NH$Dom_N_h # add schema weighting factor to schema dataframe
#Now add back the Analysis_Schema Nh and wh to site_data
site_data<-merge(site_data,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA"))
site_data[site_data$ANALYSIS_SCHEMA=="MAI_SI_A",]
site_data=site.data.gen2
site_data$GROUP=site_data$GENUS_CODE
site_data$METRIC=site_data$AdColDen
site_data$PRES.ABS=site_data$Adpres.abs
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h.as=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h.as,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
Strata_NH$Dom_N_h<-Dom_NH$Dom_N_h[match(Strata_NH$DOMAIN_SCHEMA,Dom_NH$DOMAIN_SCHEMA)]# add Dom_N_h to schema dataframe
Strata_NH$w_h<-Strata_NH$N_h/Strata_NH$Dom_N_h # add schema weighting factor to schema dataframe
nrow(site_data)
site_data.<-merge(site_data,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA"))
nrow(site_data.)
site_data.[site_data.$ANALYSIS_SCHEMA=="MAI_SI_A",]
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h.as=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h.as,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
nrow(Strata_NH)
Strata_NH<-merge(Dom_NH,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA"))
Strata_NH<-merge(Dom_NH,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA"))
nrow(Strata_NH)
#For a Given ANALYSIS_SCHEMA, we need to pool N_h, and generate w_h
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h.as=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h.as,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
Strata_NH<-merge(Dom_NH,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA")) #add Dom_N_h into Strata_NH df
Strata_NH$w_h<-Strata_NH$N_h/Strata_NH$Dom_N_h # add schema weighting factor to schema dataframe
#Now add back the Analysis_Schema Dom_N_h, Nh and wh to site_data - can't use match because we need to merge based on analysis scheme and analysis year
site_data.<-merge(site_data,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA"))
site_data.[site_data.$ANALYSIS_SCHEMA=="MAI_SI_A",]
Calc_Strata=function(site_data,grouping_field,metric_field,pres.abs_field="Adpres.abs",M_hi=250){
#Build in flexibility to look at genus or taxon level
site_data$GROUP<-site_data[,grouping_field]
#Build in flexibility to summarized different metrics
site_data$METRIC<-site_data[,metric_field]
site_data$METRIC<-as.numeric(site_data$METRIC)
site_data$PRES.ABS<-site_data[,pres.abs_field]
#For a Given ANALYSIS_SCHEMA, we need to pool N_h, and generate w_h
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h.as=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h.as,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
Strata_NH<-merge(Dom_NH,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA")) #add Dom_N_h into Strata_NH df
Strata_NH$w_h<-Strata_NH$N_h/Strata_NH$Dom_N_h # add schema weighting factor to schema dataframe
#Now add back the Analysis_Schema Dom_N_h, Nh and wh to site_data - can't use match because we need to merge based on analysis scheme and analysis year
site_data<-merge(site_data,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA"))
#Calculate summary metrics at the stratum level (rolled up from site level)
Strata_roll=ddply(site_data,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,GROUP,Dom_N_h),summarize,
n_h=length(SITE),# No. of Sites surveyed in a Strata
N_h=median(N_h.as,na.rm=T),# Strata Area (as N 50x50 grids) - median allows you to pick 1 value
w_h=median(w_h.as,na.rm=T),# weigting factor for a given stratum- median allows you to pick 1 value
D._h=mean(METRIC,na.rm=T), # Mean of Site-Level metric in a Stratum
S1_h=var(METRIC,na.rm=T), #sample variance in metric between sites
varD._h=(1-(n_h/N_h))*S1_h/n_h, #Strata level  variance of mean density
nmtot=(N_h*250), #total possible area
th=10, #minimum sampling unit
Y._h=D._h*nmtot*th,#total colony abundance in stratum **corrected using diones code
varY._h=((nmtot^2)*varD._h*(th^2)), #variance in total abundance- corrected using diones code
SE_D._h=sqrt(varD._h),
CV_D._h=(SE_D._h/D._h)*100,
SE_Y._h=sqrt(varY._h),
CV_Y._h=(SE_Y._h/Y._h)*100,
avp=sum(PRES.ABS)/n_h,
var_prop=(n_h/(n_h-1)*avp*(1-avp)),
SEprop=sqrt(var_prop))
Strata_roll$M_hi=250 #define total possible transects in a site
Strata_roll=Strata_roll[,c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA","GROUP",
"M_hi","n_h","N_h","w_h",
"D._h","S1_h","varD._h","SE_D._h","CV_D._h",
"Y._h","varY._h","SE_Y._h","CV_Y._h","avp","var_prop","SEprop")]
#remove strata that have only 1 site because you can't calculate variance
Strata_roll<-Strata_roll[Strata_roll$n_h>1,]
colnames(Strata_roll)[which(colnames(Strata_roll) == 'GROUP')] <- grouping_field #change group to whatever your grouping field is.
colnames(Strata_roll)[which(colnames(Strata_roll) == 'prop.occur')] <- pres.abs_field #change group to whatever your grouping field is.
return(Strata_roll)
}
acdG_st<-Calc_Strata(site.data.gen2,"GENUS_CODE","AdColDen","Adpres.abs")
Calc_Strata=function(site_data,grouping_field,metric_field,pres.abs_field="Adpres.abs",M_hi=250){
#Build in flexibility to look at genus or taxon level
site_data$GROUP<-site_data[,grouping_field]
#Build in flexibility to summarized different metrics
site_data$METRIC<-site_data[,metric_field]
site_data$METRIC<-as.numeric(site_data$METRIC)
site_data$PRES.ABS<-site_data[,pres.abs_field]
#For a Given ANALYSIS_SCHEMA, we need to pool N_h, and generate w_h
strat.temp<-ddply(subset(site_data,GROUP=="SSSS"),.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,NH),summarize,temp=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Strata_NH<-ddply(strat.temp,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA),summarize,N_h.as=sum(NH,na.rm=TRUE)) #calculate # of possible sites in a given stratum
Dom_NH<-ddply(Strata_NH,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA),summarize,Dom_N_h=sum(N_h.as,na.rm=TRUE))#calculate # of possible sites in a given domain, use this to calculate weighting factor
Strata_NH<-merge(Dom_NH,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA")) #add Dom_N_h into Strata_NH df
Strata_NH$w_h.as<-Strata_NH$N_h.as/Strata_NH$Dom_N_h # add schema weighting factor to schema dataframe
#Now add back the Analysis_Schema Dom_N_h, Nh and wh to site_data - can't use match because we need to merge based on analysis scheme and analysis year
site_data<-merge(site_data,Strata_NH, by= c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA"))
#Calculate summary metrics at the stratum level (rolled up from site level)
Strata_roll=ddply(site_data,.(REGION,ISLAND,ANALYSIS_YEAR,DOMAIN_SCHEMA,ANALYSIS_SCHEMA,GROUP,Dom_N_h),summarize,
n_h=length(SITE),# No. of Sites surveyed in a Strata
N_h=median(N_h.as,na.rm=T),# Strata Area (as N 50x50 grids) - median allows you to pick 1 value
w_h=median(w_h.as,na.rm=T),# weigting factor for a given stratum- median allows you to pick 1 value
D._h=mean(METRIC,na.rm=T), # Mean of Site-Level metric in a Stratum
S1_h=var(METRIC,na.rm=T), #sample variance in metric between sites
varD._h=(1-(n_h/N_h))*S1_h/n_h, #Strata level  variance of mean density
nmtot=(N_h*250), #total possible area
th=10, #minimum sampling unit
Y._h=D._h*nmtot*th,#total colony abundance in stratum **corrected using diones code
varY._h=((nmtot^2)*varD._h*(th^2)), #variance in total abundance- corrected using diones code
SE_D._h=sqrt(varD._h),
CV_D._h=(SE_D._h/D._h)*100,
SE_Y._h=sqrt(varY._h),
CV_Y._h=(SE_Y._h/Y._h)*100,
avp=sum(PRES.ABS)/n_h,
var_prop=(n_h/(n_h-1)*avp*(1-avp)),
SEprop=sqrt(var_prop))
Strata_roll$M_hi=250 #define total possible transects in a site
Strata_roll=Strata_roll[,c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA","GROUP",
"M_hi","n_h","N_h","w_h",
"D._h","S1_h","varD._h","SE_D._h","CV_D._h",
"Y._h","varY._h","SE_Y._h","CV_Y._h","avp","var_prop","SEprop")]
#remove strata that have only 1 site because you can't calculate variance
Strata_roll<-Strata_roll[Strata_roll$n_h>1,]
colnames(Strata_roll)[which(colnames(Strata_roll) == 'GROUP')] <- grouping_field #change group to whatever your grouping field is.
colnames(Strata_roll)[which(colnames(Strata_roll) == 'prop.occur')] <- pres.abs_field #change group to whatever your grouping field is.
return(Strata_roll)
}
acdG_st<-Calc_Strata(site.data.gen2,"GENUS_CODE","AdColDen","Adpres.abs")
#Create a vector of columns to subset for strata estimates
c.keep<-c("REGION","ISLAND","ANALYSIS_YEAR","ANALYSIS_SCHEMA","GENUS_CODE",
"n_h","N_h","D._h","SE_D._h","avp","SEprop","Y._h","SE_Y._h","CV_Y._h")
c.keep2<-c("REGION","ISLAND","ANALYSIS_YEAR","ANALYSIS_SCHEMA","GENUS_CODE",
"n_h","N_h","D._h","SE_D._h")
acdG_st<-Calc_Strata(site.data.gen2,"GENUS_CODE","AdColDen","Adpres.abs");acdG_st=acdG_st[,c.keep]
colnames(acdG_st)<-c("REGION","ISLAND","ANALYSIS_YEAR","Stratum","GENUS_CODE","n","Ntot","AdColDen","SE_AdColDen","Adult_avp","Adult_seprop","Adult_Abun","Adult_SE_Abun","Adult_CV")
odG_st<-Calc_Strata(site.data.gen2,"GENUS_CODE","Ave.od");odG_st=odG_st[,c.keep2]
colnames(odG_st)<-c("REGION","ISLAND","ANALYSIS_YEAR","Stratum","GENUS_CODE","n","Ntot","Ave.od","SE_Ave.od")
c.keep<-c("REGION","ISLAND","ANALYSIS_YEAR","DOMAIN_SCHEMA","ANALYSIS_SCHEMA","n_h","N_h","D._h","SE_D._h")
rich_st<-Calc_Strata_Cover_Rich(rich.data,"Richness");rich_st=rich_st[,c.keep]
colnames(rich_st)<-c("REGION","ISLAND","ANALYSIS_YEAR","Sector","Stratum","n","Ntot","Richness","SE_Richess")
#Double Check that revised pooling is adding up NH (total sites) correctly
head(acdG_st,20)
subset(sectors,ISLAND=="Kingman") #Insert whichever island/strata you want to check.
#Add in CalcAnalayis_strata & island
#Calculate Regional Estimates
acdG_is<-Calc_Domain(site.data.gen2,"GENUS_CODE","AdColDen","Adpres.abs")
acdG_is<-acdG_is[,c("ANALYSIS_YEAR","DOMAIN_SCHEMA","GENUS_CODE","n","Ntot","Mean_AdColDen","SE_AdColDen")]
odG_is<-Calc_Domain(site.data.gen2,"GENUS_CODE","Ave.od");odG_is<-odG_is[,c("ANALYSIS_YEAR","DOMAIN_SCHEMA","GENUS_CODE","n","Ntot","Mean_Ave.od","SE_Ave.od")]
rich_is<-Calc_Domain_Cover_Rich(rich.data,"Richness");colnames(rich_is)[colnames(rich_is)=="DOMAIN_SCHEMA"]<-"Sector"
MyMerge <- function(x, y){
df <- merge(x, y, by= c("REGION","ISLAND","ANALYSIS_YEAR","Stratum","GENUS_CODE","n","Ntot"), all.x= TRUE, all.y= TRUE)
return(df)
}
st.data.gen<-Reduce(MyMerge, list(acdG_st,odG_st))
colnames(st.data.gen)[colnames(st.data.gen)=="ANALYSIS_SCHEMA"]<-"Stratum"
write.csv(st.data.gen,"Demog_surveymaster_strata.csv")
MyMerge <- function(x, y){
df <- merge(x, y, by= c("ANALYSIS_YEAR","DOMAIN_SCHEMA","GENUS_CODE","n","Ntot"), all.x= TRUE, all.y= TRUE)
return(df)
}
is.data.gen<-Reduce(MyMerge, list(acdG_is,odG_is))
colnames(is.data.gen)[colnames(is.data.gen)=="DOMAIN_SCHEMA"]<-"Island"
write.csv(is.data.gen,"Demog_surveymaster_island.csv")
162*30
4860/20
160*3
550*30
8.2/20.5
20.5/22
#CREATE ADULT CLEAN ANALYSIS READY DATA----------------------------------------
# This script will clean the raw benthic REA data using method E that comes directly from the new data base application.
#Note- these data represent the revised data structure insituted in November 2018. Several recent dead and condition columns were added
rm(list=ls())
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/GIS_functions.R")
## LOAD benthic data
setwd("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/SfM")
x<-read.csv("Oahu_pilot_adult.csv")
#CREATE ADULT CLEAN ANALYSIS READY DATA----------------------------------------
# This script will clean the raw benthic REA data using method E that comes directly from the new data base application.
#Note- these data represent the revised data structure insituted in November 2018. Several recent dead and condition columns were added
rm(list=ls())
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/GIS_functions.R")
## LOAD benthic data
setwd("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/SfM")
x<-read.csv("Oahu_pilot_adults.csv")
x<-read.csv("Oahu_2019pilot_adults.csv")
head(x)
levels(x$SITE)
#CREATE ADULT CLEAN ANALYSIS READY DATA----------------------------------------
# This script will clean the raw benthic REA data using method E that comes directly from the new data base application.
#Note- these data represent the revised data structure insituted in November 2018. Several recent dead and condition columns were added
rm(list=ls())
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/GIS_functions.R")
## LOAD benthic data
setwd("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/SfM")
x<-read.csv("Oahu_2019pilot_adults.csv")
#x<-read.csv("V0_CORAL_OBS_E_final.csv")
x$SITE<-SiteNumLeadingZeros(x$SITE) # Change site number such as MAR-22 to MAR-0022
### Use these functions to look at data
head(x)
tail(x)
table(x$REGION, x$OBS_YEAR) #review years and regions in dataframe
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","OBS_YEAR",
"DATE_","NO_SURVEY_YN","EXCLUDE_FLAG","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD",
"RECENTDEAD_1","RECENT_GENERAL_CAUSE_CODE_1","RECENT_SPECIFIC_CAUSE_CODE_1",
"RECENTDEAD_2",	"RECENT_GENERAL_CAUSE_CODE_2","RECENT_SPECIFIC_CAUSE_CODE_2",
"RECENT_GENERAL_CAUSE_CODE_3","RECENT_SPECIFIC_CAUSE_CODE_3","RECENTDEAD_3","COND",
"CONDITION_2","CONDITION_3","GENUS_CODE","S_ORDER","TAXONNAME","SITE_MIN_DEPTH","SITE_MAX_DEPTH")
#remove extraneous columns
head(x[,DATA_COLS])
sort(colnames(x))
table(x$REGION) #review years and regions in dataframe
#CREATE ADULT CLEAN ANALYSIS READY DATA----------------------------------------
# This script will clean the raw benthic REA data using method E that comes directly from the new data base application.
#Note- these data represent the revised data structure insituted in November 2018. Several recent dead and condition columns were added
rm(list=ls())
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/GIS_functions.R")
## LOAD benthic data
setwd("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/SfM")
x<-read.csv("Oahu_2019pilot_adults.csv")
#x<-read.csv("V0_CORAL_OBS_E_final.csv")
x$SITE<-SiteNumLeadingZeros(x$SITE) # Change site number such as MAR-22 to MAR-0022
### Use these functions to look at data
head(x)
tail(x)
sort(colnames(x))
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","EXCLUDE_FLAG","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD",
"RECENTDEAD_1","RECENT_GENERAL_CAUSE_CODE_1","RECENT_SPECIFIC_CAUSE_CODE_1",
"RECENTDEAD_2",	"RECENT_GENERAL_CAUSE_CODE_2","RECENT_SPECIFIC_CAUSE_CODE_2",
"RECENT_GENERAL_CAUSE_CODE_3","RECENT_SPECIFIC_CAUSE_CODE_3","RECENTDEAD_3","CONDITION_1",
"CONDITION_2","CONDITION_3","GENCODE","S_ORDER","TAXONNAME","MIN_DEPTH","MAX_DEPTH")
#remove extraneous columns
head(x[,DATA_COLS])
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","EXCLUDE_FLAG","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD")
head(x[,DATA_COLS])
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","EXCLUDE_FLAG","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN")
head(x[,DATA_COLS])
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR")
head(x[,DATA_COLS])
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD",
"RECENTDEAD_1","RECENT_GENERAL_CAUSE_CODE_1","RECENT_SPECIFIC_CAUSE_CODE_1",
"RECENTDEAD_2",	"RECENT_GENERAL_CAUSE_CODE_2","RECENT_SPECIFIC_CAUSE_CODE_2",
"RECENT_GENERAL_CAUSE_CODE_3","RECENT_SPECIFIC_CAUSE_CODE_3","RECENTDEAD_3","CONDITION_1",
"CONDITION_2","CONDITION_3","GENCODE","S_ORDER","TAXONNAME","MIN_DEPTH","MAX_DEPTH")
#remove extraneous columns
head(x[,DATA_COLS])
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN")
head(x[,DATA_COLS])
#CREATE ADULT CLEAN ANALYSIS READY DATA----------------------------------------
# This script will clean the raw benthic REA data using method E that comes directly from the new data base application.
#Note- these data represent the revised data structure insituted in November 2018. Several recent dead and condition columns were added
rm(list=ls())
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/GIS_functions.R")
## LOAD benthic data
setwd("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/SfM")
x<-read.csv("Oahu_2019pilot_adults.csv")
#x<-read.csv("V0_CORAL_OBS_E_final.csv")
x$SITE<-SiteNumLeadingZeros(x$SITE) # Change site number such as MAR-22 to MAR-0022
### Use these functions to look at data
head(x)
tail(x)
sort(colnames(x))
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD")
head(x[,DATA_COLS])
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD",
"RECENTDEAD_1","RECENT_GENERAL_CAUSE_CODE_1","RECENT_SPECIFIC_CAUSE_CODE_1")
head(x[,DATA_COLS])
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD",
"RECENTDEAD_1","RECENT_GENERAL_CAUSE_CODE_1","RECENT_SPECIFIC_CAUSE_CODE_1",
"RECENTDEAD_2",	"RECENT_GENERAL_CAUSE_CODE_2","RECENT_SPECIFIC_CAUSE_CODE_2",
"RECENT_GENERAL_CAUSE_CODE_3","RECENT_SPECIFIC_CAUSE_CODE_3","CONDITION_1",
"CONDITION_2","CONDITION_3","GENCODE","S_ORDER","TAXONNAME","MIN_DEPTH","MAX_DEPTH")
#remove extraneous columns
head(x[,DATA_COLS])
#CREATE ADULT CLEAN ANALYSIS READY DATA----------------------------------------
# This script will clean the raw benthic REA data using method E that comes directly from the new data base application.
#Note- these data represent the revised data structure insituted in November 2018. Several recent dead and condition columns were added
rm(list=ls())
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/GIS_functions.R")
## LOAD benthic data
setwd("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/SfM")
x<-read.csv("Oahu_2019pilot_adults.csv")
#x<-read.csv("V0_CORAL_OBS_E_final.csv")
x$SITE<-SiteNumLeadingZeros(x$SITE) # Change site number such as MAR-22 to MAR-0022
### Use these functions to look at data
head(x)
tail(x)
sort(colnames(x))
#Create vector of column names to include then exclude unwanted columns from dataframe
DATA_COLS<-c("MISSIONID","REGION","REGION_NAME","ISLAND","ISLANDCODE","SITE","LATITUDE",	"LONGITUDE","REEF_ZONE","DEPTH_BIN","YEAR",
"DATE_","NO_SURVEY","SITEVISITID","HABITAT_CODE","DIVER","TRANSECTNUM","SEGMENT","SEGWIDTH","SEGLENGTH","FRAGMENT_YN",
"COLONYID","TAXONCODE","COLONYLENGTH","OLDDEAD",
"RECENTDEAD_1","RECENT_GENERAL_CAUSE_CODE_1","RECENT_SPECIFIC_CAUSE_CODE_1",
"RECENTDEAD_2",	"RECENT_GENERAL_CAUSE_CODE_2","RECENT_SPECIFIC_CAUSE_CODE_2",
"RECENT_GENERAL_CAUSE_CODE_3","RECENT_SPECIFIC_CAUSE_CODE_3","CONDITION_1",
"CONDITION_2","CONDITION_3","GENCODE","S_ORDER","TAXONNAME","MIN_DEPTH","MAX_DEPTH")
#remove extraneous columns
head(x[,DATA_COLS])
x<-x[,DATA_COLS]
sort(colnames(x))
#Double check level and class of variables to make sure there aren't any errors
sapply(x,levels)
sapply(x,class)##Change column names to make code easier to code
colnames(x)[colnames(x)=="TAXONCODE"]<-"SPCODE" #Change column name
colnames(x)[colnames(x)=="TRANSECTNUM"]<-"TRANSECT" #Change column name
colnames(x)[colnames(x)=="RECENTDEAD_1"]<-"RDEXTENT1" #Change column name
colnames(x)[colnames(x)=="RECENT_GENERAL_CAUSE_CODE_1"]<-"GENRD1" #Change column name
colnames(x)[colnames(x)=="RECENT_SPECIFIC_CAUSE_CODE_1"]<-"RD1" #Change column name
colnames(x)[colnames(x)=="RECENTDEAD_2"]<-"RDEXTENT2" #Change column name
colnames(x)[colnames(x)=="RECENT_GENERAL_CAUSE_CODE_2"]<-"GENRD2" #Change column name
colnames(x)[colnames(x)=="RECENT_SPECIFIC_CAUSE_CODE_2"]<-"RD2" #Change column name
colnames(x)[colnames(x)=="RECENT_GENERAL_CAUSE_CODE_3"]<-"GENRD3" #Change column name
colnames(x)[colnames(x)=="RECENT_SPECIFIC_CAUSE_CODE_3"]<-"RD3" #Change column name
colnames(x)[colnames(x)=="FRAGMENT_YN"]<-"Fragment" #Change column name
head(x)
